{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a5ca820",
   "metadata": {},
   "source": [
    "# Experiments on Version 2\n",
    "## of Adaptive Dropout Probabilities via Tsetlin Machine-inspired Majority Voting\n",
    "\n",
    "In this notebook I introduce the new version, and continue experiments on \"tsetlin\" dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c670303",
   "metadata": {},
   "source": [
    "For background and history, check out the notebook titled \"Initial Experiments\". \n",
    "\n",
    "Version 2! This version addresses some of the problems that the first draft had, namely speed and variance. As a refresher, the initial version assigned each unit/input an individual dropout probability that was updated after each gradient update. The probability of being included in the network would decrease as the network's accuracy increased, and vice-versa. Specifically, for each classification in a batch, the probabilities of the units that were used in the classification were updated by the following formulae:\n",
    "\n",
    "$$\n",
    "{p}_{t+1} = p_{t} - \\alpha (p_{t}-1)*exp(-p_{t})  \n",
    "$$\n",
    "\n",
    "For incorrect classification, and:\n",
    "\n",
    "$$\n",
    "{p}_{t+1} = p_{t} - \\alpha (p_{t})*exp(p_{t}-1) \n",
    "$$\n",
    "\n",
    "for correct classification. This remains true for version 2, however instead of updating the probabilites for every classification in the batch, version 2 counts the number of correct vs incorrect classifications, and updates the probabilities once in the direction of the majority. This reduces computation and variance while keeping the spirit of the original version. The key philosophy is to adapt the dropout probabilities w.r.t network performance, and this version still does that! If I really wanted to lean into the Tsetlin Machine inspiriation, I could add hyperparameter that changes the threshold from simple majority to some other ratio, but for now, let's look at this version. \n",
    "\n",
    "In code, it looks like this: \n",
    "Note the addition of the clipping (tor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f891b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TsetlinUnitDropout(nn.Module):  # Drops units rather than weights\n",
    "    def __init__(self, in_size, init_prob, step_discount, clip=False, clip_min=None, clip_max=None):\n",
    "        super().__init__()\n",
    "        assert 0 <= init_prob < 1\n",
    "        self.discount = step_discount\n",
    "        self.pmin = clip_min\n",
    "        self.pmax = clip_max\n",
    "        self.clip = clip\n",
    "        self.probabilities = torch.nn.Parameter(torch.tensor(np.full((1, in_size), init_prob, dtype=\"float32\")),\n",
    "                                                requires_grad=False)  # Should these be no grad parameters?\n",
    "        self.not_dropped = []  # this one too? Also is there something better than empty list?\n",
    "\n",
    "    def forward(self, inp):\n",
    "        assert inp.shape[1] == self.probabilities.shape[1]\n",
    "        dev = self.probabilities.device\n",
    "        comparator = torch.rand(inp.shape[1]).to(dev)\n",
    "        mask = (comparator < self.probabilities).float().to(dev) # compares rand nums to probability of being\n",
    "        # included\n",
    "        # Assign not_dropped indices of prob tensor whose unit WAS included in network (These will be updated at step)\n",
    "        self.not_dropped = mask.nonzero(as_tuple=True)  # To be indexed with\n",
    "        return mask * inp / self.probabilities  # inverted dropout scaling\n",
    "\n",
    "    def tsetlin_update(self, correct_list):  # This implementation penalizes (increases dropout prob) if correct\n",
    "            batch_size = len(correct_list)\n",
    "            corr_distance = 2*correct_list.sum() - batch_size\n",
    "            if corr_distance > 0: # decrease chance of inclusion (increase dropout prob) if correct\n",
    "              self.probabilities[self.not_dropped] -=  (self.probabilities[self.not_dropped] * self.discount / torch.exp(\n",
    "                    1 - self.probabilities[self.not_dropped]))\n",
    "\n",
    "            elif corr_distance < 0:  # this number is negative (-= - = +) ie. increase chance of inclusion (decrease dropout prob)\n",
    "                self.probabilities[self.not_dropped] -=  (self.discount * (self.probabilities[self.not_dropped] - 1) / torch.exp(\n",
    "                    self.probabilities[self.not_dropped]))\n",
    "\n",
    "            if self.clip:\n",
    "              torch.clamp_(self.probabilities, self.pmin, self.pmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8536c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
